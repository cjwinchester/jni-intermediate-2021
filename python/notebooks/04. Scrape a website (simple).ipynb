{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3119c6",
   "metadata": {},
   "source": [
    "# Scrape a website (simple)\n",
    "\n",
    "This notebook has code to scrape the 12 most recent alerts from the [Queensland Workplace Health and Safety website](https://www.worksafe.qld.gov.au/news-and-events/alerts) and write the data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the convention is to put imports at the top of your script --\n",
    "specifically, to put imports from the standard library first, then a\n",
    "blank line, then imports from any third-party libraries you've installed\n",
    "'''\n",
    "\n",
    "# the standard library module for working with tabular data\n",
    "import csv\n",
    "\n",
    "# third-party library for handling HTTP traffic: https://docs.python-requests.org/en/master/\n",
    "import requests\n",
    "# third-party library for parsing strings of HTML into Python data objects: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46bbcb",
   "metadata": {},
   "source": [
    "## Step 1: Request the page\n",
    "\n",
    "1. Call the `requests.get()` method to request the page we're interested in, and save the results of that operation to a new variable\n",
    "2. Verify that it worked by checking the `text` attribute of the object returned -- this is the HTML underlying the page we're scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the requests.get() method to request the page \n",
    "req = requests.get('https://www.worksafe.qld.gov.au/news-and-events/alerts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff901a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the .text attribute of the request, which is the HTML code behind the page\n",
    "print(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2862e6d",
   "metadata": {},
   "source": [
    "## Step 2: Parse the HTML\n",
    "\n",
    "1. Feed that HTML string into a new BeautifulSoup object.\n",
    "2. Poke through the HTML on the page to figure out the elements we need to target -- Chrome/Firefox browser tools are great for this.\n",
    "3. Having found the alerts in an unordered list (`<ul>`) element that has a class attribute of `search-results__grid`, use BeautifulSoup's `find()` method to extract it from the larger tree of HTML elements.\n",
    "4. Use BeautifulSoup's `find_all()` method to extract all of the list items (`<li>`) inside the unordered list we just extracted -- this will return multiple elements stored in a Python _list_.\n",
    "5. Using a Python [for loop](https://docs.python.org/3/tutorial/controlflow.html#for-statements), loop over each list item and extract the pieces of data we want to capture for each alert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse that string of HTML into a BeautifulSoup object using the default\n",
    "# 'html.parser' parsing engine\n",
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cde10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the find() method to isolate the unordered list (ul)\n",
    "# that contains the alerts\n",
    "results_list = soup.find('ul', {'class': 'search-results__grid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within that ul, use the find_all() method to retrieve a list of\n",
    "# list items (li)\n",
    "results_items = results_list.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see how many items in the list\n",
    "print(len(results_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the list of items\n",
    "for item in results_items:\n",
    "    \n",
    "    # find() the h4 level header within this list item\n",
    "    header = item.find('h4')\n",
    "    \n",
    "    # grab the text from that header and strip any whitespace\n",
    "    header_text = header.text.strip()\n",
    "    \n",
    "    # find() the anchor tag ('a') in the header and access the 'href' link attribute\n",
    "    link = header.find('a')['href']\n",
    "    \n",
    "    # find() the paragraph tag, access the text attribute and strip whitespace\n",
    "    description = item.find('p').text.strip()\n",
    "    \n",
    "    # find() the date span and grab the stripped text\n",
    "    date = item.find('span', {'class': 'single-date'}).text.strip()\n",
    "    \n",
    "    # print everything we captured with a newline break between each thing\n",
    "    print(date, header_text, link, description, sep='\\n')\n",
    "    \n",
    "    # and a blank line to make things easier to see\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb837e5",
   "metadata": {},
   "source": [
    "## Step 3: Write to file\n",
    "\n",
    "Next, instead of just printing the pieces of data we've targeted, we'll write them to file using the `csv` package, which is part of Python's standard library.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Note**: Typically, you'd write this code in the same loop you wrote above, just replacing the print statements with instructions to write to file, but for the purposes of this workshop we're breaking this into two code blocks. Generally, the process involves writing your code step by step, stopping frequently to test that you're capturing the data correctly, then once everything looks good, adapting the code to write the data to file (or whatever your output is).\n",
    "\n",
    "---\n",
    "\n",
    "1. Inside a `with` block, open a file to write the data into in write (`w`) mode.\n",
    "2. Create a CSV writer object and write the headers to file.\n",
    "3. Loop over the items as before, targeting the pieces of data to write out.\n",
    "4. Instead of printing them, drop them into a Python list and write to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a new CSV file in write mode\n",
    "with open('qld-workplace-alerts-latest.csv', 'w') as outfile:\n",
    "    \n",
    "    # create a writer object\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # make a list of headers for our CSV\n",
    "    headers = ['date', 'hed', 'description', 'link']\n",
    "    \n",
    "    # ... and write them to file\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # loop over the list of items\n",
    "    for item in results_items:\n",
    "\n",
    "        # find() the h4 level header within this list item\n",
    "        header = item.find('h4')\n",
    "\n",
    "        # grab the text from that header and strip any whitespace\n",
    "        header_text = header.text.strip()\n",
    "\n",
    "        # find() the anchor tag ('a') in the header and access the 'href' link attribute\n",
    "        link = header.find('a')['href']\n",
    "\n",
    "        # find() the paragraph tag, access the text attribute and strip whitespace\n",
    "        description = item.find('p').text.strip()\n",
    "\n",
    "        # find() the date span and grab the stripped text\n",
    "        date = item.find('span', {'class': 'single-date'}).text.strip()\n",
    "\n",
    "        # build a list of data to write out to file\n",
    "        data_to_write = [date, header_text, description, link]\n",
    "\n",
    "        # ... and write to file\n",
    "        writer.writerow(data_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0307da",
   "metadata": {},
   "source": [
    "## üö® Extra credit: Parse dates\n",
    "\n",
    "Use the Python standard library's `datetime.datetime` module -- specifically, its `strptime()` method -- to convert the date string for each entry (\"1 April 2020\") into a Python date object as you loop over the items.\n",
    "\n",
    "Here's an example of how to do this:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "test_date = \"1 April 2020\"\n",
    "\n",
    "date_object = datetime.strptime(test_date, \"%d %B %Y\")\n",
    "\n",
    "print(date_object)\n",
    "```\n",
    "\n",
    "[Here's a handy website to bookmark](https://strftime.org/) with all of Python's date/time formatting directives.\n",
    "\n",
    "[And here's a link to the Python documentation on the `strftime()` and `strptime()` methods](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1bb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6c6171",
   "metadata": {},
   "source": [
    "## üö® Extra extra credit: Send your output elsewhere\n",
    "\n",
    "Instead of just writing this data to file, you could also:\n",
    "- Send newsworthy alerts to a Twitter bot using [Tweepy](https://www.tweepy.org/) or another Python wrapper over the Twitter API\n",
    "- Send new alerts to a Slack channel using the official [Python SDK](https://slack.dev/python-slack-sdk/) (Software Development Kit) or by setting up [a webhook URL](https://api.slack.com/messaging/webhooks) and using the `requests` library to POST your message data to that URL\n",
    "- Ingest your new CSV data into a data analysis workflow using [pandas](pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc74ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
