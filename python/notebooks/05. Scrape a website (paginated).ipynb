{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164285e4",
   "metadata": {},
   "source": [
    "# Scrape a website (paginated)\n",
    "\n",
    "This notebook has code to scrape every page of alerts from the [Queensland Workplace Health and Safety website](https://www.worksafe.qld.gov.au/news-and-events/alerts) and write the data to a CSV file.\n",
    "\n",
    "We will reuse the core extraction logic of our simple scraper but extend the script to capture multiple pages. The steps will be:\n",
    "1. Fetch the initial landing page and parse the HTML to determine how many pages we need to spin through\n",
    "2. Request each page in turn and download a copy onto the computer\n",
    "3. Open each downloaded page and parse the HTML into data to write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the convention is to put imports at the top of your script --\n",
    "specifically, to put imports from the standard library first, then a\n",
    "blank line, then imports from any third-party libraries you've installed\n",
    "'''\n",
    "\n",
    "# the standard library module for working with tabular data\n",
    "import csv\n",
    "# a standard library module for dealing with time-based tasks -- in\n",
    "# our case, to \"sleep\" for a second between requests\n",
    "# https://docs.python.org/3/library/time.html#time.sleep\n",
    "import time\n",
    "# a standard library module to run local file searches\n",
    "import glob\n",
    "\n",
    "\n",
    "# third-party library for handling HTTP traffic: https://docs.python-requests.org/en/master/\n",
    "import requests\n",
    "# third-party library for parsing strings of HTML into Python data objects: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c25f03",
   "metadata": {},
   "source": [
    "## Step 1: Request the base page and figure out how many pages to grab\n",
    "\n",
    "First, we're going to fetch the first page of alerts so we can see how many pages we need to fetch. Looking at how the URL changes as you page through the results, you can see that the [query parameter](https://en.wikipedia.org/wiki/Query_string) that changes is `start_rank` -- the offset.\n",
    "\n",
    "Each page shows 12 results, so [page 2](https://www.worksafe.qld.gov.au/news-and-events/alerts?start_rank=13) has a `start_rank` of 13, [page 3](https://www.worksafe.qld.gov.au/news-and-events/alerts?start_rank=25) has a `start_rank` of 25, and so on. That's going to be the key pattern we'll hook into to capture all of the pages.\n",
    "\n",
    "The pagination controls are at the bottom of the page. You'll notice a link that allows you to jump to the last page of results -- that link will answer our question of \"how many pages do we need to visit to get everything?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the base page\n",
    "req = requests.get('https://www.worksafe.qld.gov.au/news-and-events/alerts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the HTML into soup\n",
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target the element that has the link to the last page\n",
    "last_page = soup.find('li', {'class': 'pagination__item--last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdf39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target the anchor tag (a) and grab the 'href' link attribute\n",
    "last_page_link = last_page.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb835287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the split() function to grab just the value for the final page start_rank\n",
    "# and pass it to the int() function to turn the string into a number\n",
    "last_page_offset = int(last_page_link.split('start_rank=')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eccdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_page_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b39a0d",
   "metadata": {},
   "source": [
    "## Step 2: Request the pages and save them to file\n",
    "\n",
    "We're going to save each page into a folder called `scraped-pages` (I set this up ahead of time). Why? This way, if our script crashes partway through, we'll at least have those pages cached and can pick up where we left off.\n",
    "\n",
    "The process:\n",
    "1. Generate the URL for each page based on its offset using a [range()](https://docs.python.org/3/library/stdtypes.html#range) with an offset of 12\n",
    "2. Retrieve each page and save the HTML contents to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12831a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many items on each page?\n",
    "items_per_page = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the URL template with only the offset value missing\n",
    "url_pattern_base = 'https://www.worksafe.qld.gov.au/news-and-events/alerts?start_rank='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the folder where our files will land\n",
    "save_folder = 'scraped-pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a110787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out the logic for the range to make sure the `start_rank` params will be correct\n",
    "# we have to grab the last page offset PLUS ONE for the second value\n",
    "# because ranges are exclusive at the top end of the range --\n",
    "# more details: https://docs.python.org/3/library/stdtypes.html#range\n",
    "for i in range(1, last_page_offset+1, items_per_page):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we know the query parameters are generated correctly,\n",
    "# we can weave in the actual \"grab this page and download it\" code\n",
    "\n",
    "# loop over the range\n",
    "for i in range(1, last_page_offset+1, items_per_page):\n",
    "    \n",
    "    # build the URL using an f-string\n",
    "    # https://docs.python.org/3/tutorial/inputoutput.html#tut-f-strings\n",
    "    url = f'{url_pattern_base}{i}'\n",
    "    \n",
    "    # define the filepath for where we're going to save this page\n",
    "    filepath = f'{save_folder}/qld-workplace-alerts-{i}.html'\n",
    "    \n",
    "    # fetch the page\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    # save to file\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write(req.text)\n",
    "    \n",
    "    # let us know what's up\n",
    "    print(filepath)\n",
    "    \n",
    "    # wait a tick\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29c141",
   "metadata": {},
   "source": [
    "## Step 2: Parse the pages and extract the data\n",
    "\n",
    "Next, we'll open those files that we downloaded onto our computer and extract the data using the same logic we used in our previous scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6821930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to all those files with the glob module\n",
    "# https://docs.python.org/3/library/glob.html\n",
    "\n",
    "files = glob.glob(f'{save_folder}/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97482be",
   "metadata": {},
   "source": [
    "## Step 3: Parse the data and write to file\n",
    "\n",
    "In this example, we're doing everything in one fell swoop:\n",
    "- Open each HTML file I saved onto my computer\n",
    "- Parse the HTML and target the data we want\n",
    "- Write that data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qld-workplace-alerts-all.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # make a list of headers for our CSV\n",
    "    headers = ['date', 'hed', 'description', 'link']\n",
    "    \n",
    "    # ... and write them to file\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # now go through each file we downloaded\n",
    "    for file in files:\n",
    "        with open(file, 'r') as infile:\n",
    "            html = infile.read()\n",
    "\n",
    "        # parse that string of HTML into a BeautifulSoup object using the default\n",
    "        # 'html.parser' parsing engine\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        # use the find() method to isolate the unordered list (ul)\n",
    "        # that contains the alerts\n",
    "        results_list = soup.find('ul', {'class': 'search-results__grid'})\n",
    "\n",
    "        # within that ul, use the find_all() method to retrieve a list of\n",
    "        # list items (li)\n",
    "        results_items = results_list.find_all('li')\n",
    "\n",
    "        # loop over the list of items\n",
    "        for item in results_items:\n",
    "\n",
    "            # find() the h4 level header within this list item\n",
    "            header = item.find('h4')\n",
    "\n",
    "            # grab the text from that header and strip any whitespace\n",
    "            header_text = header.text.strip()\n",
    "\n",
    "            # find() the anchor tag ('a') in the header and access the 'href' link attribute\n",
    "            link = header.find('a')['href']\n",
    "\n",
    "            # find() the paragraph tag, access the text attribute and strip whitespace\n",
    "            description = item.find('p').text.strip()\n",
    "\n",
    "            # find() the date span and grab the stripped text\n",
    "            date = item.find('span', {'class': 'single-date'}).text.strip()\n",
    "\n",
    "            # build a list of data to write out to file\n",
    "            data_to_write = [date, header_text, description, link]\n",
    "\n",
    "            # ... and write to file\n",
    "            writer.writerow(data_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076eecb",
   "metadata": {},
   "source": [
    "## 🚨 Extra credit: Write a function\n",
    "\n",
    "So there's this concept in computer programming called [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) -- Don't Repeat Yourself -- and the general idea is, if you find yourself copying and pasting a bunch of the same code into different contexts, it might be time to extract that bit of logic into a reusable function.\n",
    "\n",
    "Python comes with a bunch of handy built-in functions, like `print()`, but you can also define your own functions that take inputs and return outputs. You can check out some more explanation and examples in the Functions notebook in the `reference-notebooks` folder.\n",
    "\n",
    "In this case, it might be useful to define a function that accepts a string of HTML from one of the pages of alert results as an input, parses that HTML with BeautifulSoup, extracts the target data and returns the list of data. Then you could _call_ that function at any step in your script where you have a hunk of HTML you need parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0fcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
