{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data in pandas\n",
    "\n",
    "For cleaning jobs of any size, specialized tools like [OpenRefine](http://openrefine.org/) are still your best bet -- a typical workflow is to clean your data in OpenRefine, export as a CSV, then load into pandas.\n",
    "\n",
    "But in many cases, you can use some of pandas' built-in tools to whip your data into shape. This is especially useful for data processing tasks that you plan to repeat as the data are updated.\n",
    "\n",
    "Let's import pandas, then we'll run through some scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How dirty is your data?\n",
    "\n",
    "In Excel, running a pivot table (with counts) for each column will show you misspellings, external white space, inconsistent casing and other problems that keep your data from grouping correctly.\n",
    "\n",
    "In SQL, you might do the same thing with The Golden Query‚Ñ¢Ô∏è:\n",
    "\n",
    "```sql\n",
    "SELECT column, COUNT(*)\n",
    "FROM table\n",
    "GROUP BY column\n",
    "ORDER BY 2 DESC\n",
    "```\n",
    "\n",
    "To do the equivalent operation in pandas, you can just call the `value_counts()` method on a column. Let's look at some Congressional junkets data as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets = pd.read_csv('../data/congress_junkets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `value_counts()` on the _Destination_ colummn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default sort order is by count descending, but it can also be helpful in finding typos to sort by the name -- the \"index\" of what `value_counts()` returns. To do that, tack on `sort_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets['Destination'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now we start to see some common data problems in our 838 unique destinations -- whitespace, inconsistent values for the same thing (\"Accra\" and \"Accra, Ghana\") -- and can start fixing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing whitespace, casing and other \"string\" problems\n",
    "\n",
    "If part of our analysis hinged on having a pristine \"Destination\" column, then we've got some work ahead of us. First thing I'd do: Strip whitespace and upcase the text.\n",
    "\n",
    "You can do a lot of basic cleanup like this by applying Python's built-in string methods to the `str` attribute of a column.\n",
    "\n",
    "üëâ For more information on Python string methods, [check out this notebook](Data%20types%20and%20basic%20syntax.ipynb#String-methods).\n",
    "\n",
    "To start with, let's create a new column, `destination_clean`, with a stripped/uppercase version of the destination data.\n",
    "\n",
    "**Note**: Outside of pandas, you can use \"method chaining\" to apply multiple transformations to a string, like this: `'   My String'.upper().strip()`.\n",
    "\n",
    "When you're chaining string methods on the `str` attribute of a pandas column series, though, it doesn't work like that -- you have to call `str` after each method call. In other words:\n",
    "\n",
    "```python\n",
    "# this will throw an error\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().strip()\n",
    "\n",
    "# this will work\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets['destination_clean'] = junkets['Destination'].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `value_counts()` again to see if that helped at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets['destination_clean'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That eliminated a handful of problems. Now comes the tedious work of identifying entries to find and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk-replacing values with other values\n",
    "\n",
    "If we were at this point in Excel, we'd scroll through the list of unique names and start making notes of what we need to change. Same story here.\n",
    "\n",
    "Let's loop over a [sorted](https://docs.python.org/3/howto/sorting.html) list of `unique()` destinations and `print()` each one.\n",
    "\n",
    "üëâ For a refresher on _for loops_, [see this notebook](Data%20types%20and%20basic%20syntax.ipynb#for-loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for destination in sorted(junkets.destination_clean.unique()):\n",
    "    print(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is where we're going to start encoding our editorial choices. \"Ames, IA\" or \"Ames, Iowa\"? \"Baku, Azerjaijan,\" or \"Baku, Republic of Azerbaijan\"? Etc.\n",
    "\n",
    "There are several ways we could structure this data, but a dictionary sounds like it'd be the most fun, so let's do that. Each key will be a string that we'd like to replace; each value will be the string we'd like to replace it with. To get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_fixes = {\n",
    "    'BAKU, AZERBIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'BAKU, REPUBLIC OF AZERBAIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'ADDIS, ETHIOPIA': 'ADDIS ABABA, ETHIOPIA',\n",
    "    'ANKEY, IA': 'ANKENY, IA'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and so on. (This is tedious work, and -- again -- tools like OpenRefine make this process somewhat less tedious. But if you have a long-term project that involves data that will be updated regularly, and it's worth putting in the time to make sure the data are cleaned the same way each time, you can do it all in pandas.)\n",
    "\n",
    "üëâ For more information on dictionaries, [check out this notebook](Data%20types%20and%20basic%20syntax.ipynb#Dictionaries).\n",
    "\n",
    "Here's how we might _apply_ our bulk find-and-replace dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replace_destination(row):\n",
    "    '''Given a row of data, see if the value is a typo to be replaced'''\n",
    "    \n",
    "    # get the clean destination value\n",
    "    dest = row['destination_clean']\n",
    "    \n",
    "    # try to look it up in the `typo_fixes` dictionary\n",
    "    # the `get()` method will return None if it's not there\n",
    "    typo = typo_fixes.get(dest)\n",
    "    \n",
    "    # then we can test to see if `get()` got an item out of the dictionary (True)\n",
    "    # or if it returned None (False)\n",
    "    if typo:\n",
    "        # if it found an entry in our dictionary,\n",
    "        # return the value from that key/value pair\n",
    "        return typo_fixes[dest]\n",
    "    # otherwise\n",
    "    else:\n",
    "        # return the original destination string\n",
    "        return dest\n",
    "\n",
    "# apply the function and overwrite our working \"clean' column\"\n",
    "junkets['destination_clean'] = junkets.apply(find_replace_destination, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ For more information on writing your own functions, [check out this notebook](Functions.ipynb).\n",
    "\n",
    "üëâ For more information on applying functions to a pandas data frame, [check out this notebook](Pandas%20-%20Using%20the%20apply%20method.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonstandard values to represent null entries\n",
    "\n",
    "Data creators may express null values in a variety of ways -- `''`, `'n/a'`, `NA`, `.`, etc. But for your purposes, you want pandas to read them all as `NaN`, so you can take advantage of methods like [`isnull()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html) in your analysis.\n",
    "\n",
    "If you've already done some exploratory analysis, you can specify the `na_values` argument when you read in the data -- you can supply a _single_ value that should be interpreted as null, or you can hand off a list `[]` of values.\n",
    "\n",
    "As an example, let's take a look at some EPA air quality data from Ohio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_excel('../data/epa.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conferring with the source of this data, the dots `.` represent \"no observation\" -- a null value. Let's try reading that in again, this time specifying `na_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_excel('../data/epa.xlsx',\n",
    "                            na_values='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You want to replace null values with 0, or something else\n",
    "\n",
    "Use the [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method on a data frame or column series to fill null values with some other value.\n",
    "\n",
    "Let's say our reporting had shown that the dots in the air quality data weren't, in fact null. Let's say they were actually supposed to be zeroes. Here's how we'd fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have duplicate rows\n",
    "\n",
    "If your data have rows that are incorrectly duplicated, you use the data frame method [`drop_duplicates()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html) to delete the duplicates.\n",
    "\n",
    "(This assumes, of course, that you've done sufficient reporting to feel confident that the duplicated rows aren't in there legitimately.)\n",
    "\n",
    "Let's look at some fake data to show how this'd work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = [\n",
    "    {'id': 12345, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 54321, 'name': 'George', 'position': 'Reporter', 'org': 'Some Other News Organization'},\n",
    "    {'id': 12345, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 49382, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 39331, 'name': 'Pat', 'position': 'Producer', 'org': 'Some Other Other News Organization'},\n",
    "]\n",
    "\n",
    "fake_df = pd.DataFrame(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you drop anything, you'd probably want to check for duplicate rows. You can do that by filtering your data using the `duplicated()` method.\n",
    "\n",
    "üëâ For more details on filtering data in pandas, [see this notebook](Pandas%20-%20Filtering%20columns%20and%20rows.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df[fake_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is showing us a row where every value in every column matches exactly the values in at least one other row. So we've done the reporting to show that we need to cut the duplicates here.\n",
    "\n",
    "The `drop_duplicates()` method gives you control over _how_ this happens:\n",
    "- You can drop _all_ duplicate rows, or keep just the first instance (this is the default behavior), or the last instance\n",
    "- You can drop rows where just the values in certain columns are duplicated\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default behavior -- duplicate rows must match exactly\n",
    "fake_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where the values in name, org and position are identical\n",
    "fake_df.drop_duplicates(subset=['name', 'org', 'position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original data frame is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because we didn't specify `in_place=True` as an argument.\n",
    "\n",
    "You can take one of two approaches here. You could alter your original dataframe -- the code would look like this:\n",
    "\n",
    "```python\n",
    "# drop rows where the values in name, org and position are identical\n",
    "fake_df.drop_duplicates(subset=['name', 'org', 'position'], inplace=True)\n",
    "```\n",
    "\n",
    "-- or you could \"save\" the resulting deduplicated data frame as a new variable, like this:\n",
    "\n",
    "```python\n",
    "# drop rows where the values in name, org and position are identical\n",
    "deduped = fake_df.drop_duplicates(subset=['name', 'org', 'position'])\n",
    "```\n",
    "\n",
    "I prefer the latter approach because I like to leave the original data as untouched as possible, working up to successively cleaner and more analyze-able data frames as I go. I also find this approach is easier to follow when I come back to it after a few weeks or months of inaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have empty rows\n",
    "\n",
    "To drop rows or columns whose values are all `NA`, use [`dropna()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html).\n",
    "\n",
    "Specifying `axis=1` will drop empty _columns_; `axis=0` will drop empty _rows_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_df = fake_df.append({'id': np.nan, 'name': np.nan, 'position': np.nan, 'org': np.nan}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "This just scratches the surface of what you can do in pandas. Here are some other resources to check out:\n",
    "\n",
    "- [Pythonic Data Cleaning With NumPy and Pandas](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
    "- [pandas official list of tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "- [Karrie Kehoe's guide to cleaning data in pandas](https://github.com/KarrieK/pandas_data_cleaning)\n",
    "- [Data cleaning with Python](https://www.dataquest.io/blog/data-cleaning-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
